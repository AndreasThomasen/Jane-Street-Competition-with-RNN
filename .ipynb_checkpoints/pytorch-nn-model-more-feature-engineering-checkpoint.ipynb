{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015493,
     "end_time": "2021-02-08T04:54:31.783533",
     "exception": false,
     "start_time": "2021-02-08T04:54:31.768040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simple Predictor for Jane-Street Market competition\n",
    "\n",
    "Here I show how to build a very simple neural network model in Pytorch and train it on a GPU.\n",
    "\n",
    "Pre-processing of data is done as described in [this notebook](https://www.kaggle.com/andreasthomasen/preprocessing-and-feature-selection). The main difference is that we only do PCA here and retain a lot of features. The reason is that we do not use RNNs, but instead only rely on instantaneous feature values. So this model can be trained with quite a lot of features included.\n",
    "\n",
    "If you read this notebook from start to finish, you will learn how to\n",
    "* Load data into pandas\n",
    "* Do feature reduction using PCA\n",
    "* Define a neural network model in pytorch\n",
    "* Train the model and save it using pickle\n",
    "\n",
    "Thanks for reading, if you like it, feel free to copy it. Nothing revolutionary in this notebook.\n",
    "\n",
    "UPDATE: Including the training step, it took too long to run this notebook for submission. So instead it now saves the model at the end. You can run it later in a private submission.\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-08T04:54:31.822068Z",
     "iopub.status.busy": "2021-02-08T04:54:31.821494Z",
     "iopub.status.idle": "2021-02-08T04:54:34.439836Z",
     "shell.execute_reply": "2021-02-08T04:54:34.440418Z"
    },
    "papermill": {
     "duration": 2.642364,
     "end_time": "2021-02-08T04:54:34.440618",
     "exception": false,
     "start_time": "2021-02-08T04:54:31.798254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\n",
      "/kaggle/input/jane-street-market-prediction/features.csv\n",
      "/kaggle/input/jane-street-market-prediction/example_test.csv\n",
      "/kaggle/input/jane-street-market-prediction/train.csv\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "\n",
    "import pickle\n",
    "    \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014769,
     "end_time": "2021-02-08T04:54:34.471755",
     "exception": false,
     "start_time": "2021-02-08T04:54:34.456986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load data and reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-02-08T04:54:34.508788Z",
     "iopub.status.busy": "2021-02-08T04:54:34.508152Z",
     "iopub.status.idle": "2021-02-08T04:56:53.959240Z",
     "shell.execute_reply": "2021-02-08T04:56:53.958139Z"
    },
    "papermill": {
     "duration": 139.472553,
     "end_time": "2021-02-08T04:56:53.959392",
     "exception": false,
     "start_time": "2021-02-08T04:54:34.486839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\n",
    "batch_size = len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015057,
     "end_time": "2021-02-08T04:56:53.990268",
     "exception": false,
     "start_time": "2021-02-08T04:56:53.975211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In our predictive modeling we will train the network to return the action. Let's create columns for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:56:54.035142Z",
     "iopub.status.busy": "2021-02-08T04:56:54.034359Z",
     "iopub.status.idle": "2021-02-08T04:56:57.790392Z",
     "shell.execute_reply": "2021-02-08T04:56:57.789604Z"
    },
    "papermill": {
     "duration": 3.785036,
     "end_time": "2021-02-08T04:56:57.790500",
     "exception": false,
     "start_time": "2021-02-08T04:56:54.005464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['act'] = (train['resp'] > 0).astype('int')\n",
    "train['act_1'] = (train['resp_1'] > 0).astype('int')\n",
    "train['act_2'] = (train['resp_2'] > 0).astype('int')\n",
    "train['act_3'] = (train['resp_3'] > 0).astype('int')\n",
    "train['act_4'] = (train['resp_4'] > 0).astype('int')\n",
    "target = torch.tensor(train[['act','act_1','act_2','act_3','act_4']].to_numpy(),dtype=torch.float,device=dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01516,
     "end_time": "2021-02-08T04:56:57.821050",
     "exception": false,
     "start_time": "2021-02-08T04:56:57.805890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The tensors below will be used later. The wrtensor is used in training. We store feature_0 in a separate tensor since it is the only integer valued feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:56:57.861567Z",
     "iopub.status.busy": "2021-02-08T04:56:57.860796Z",
     "iopub.status.idle": "2021-02-08T04:56:57.925434Z",
     "shell.execute_reply": "2021-02-08T04:56:57.924932Z"
    },
    "papermill": {
     "duration": 0.089269,
     "end_time": "2021-02-08T04:56:57.925538",
     "exception": false,
     "start_time": "2021-02-08T04:56:57.836269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_0 = train['feature_0']\n",
    "itensor = torch.tensor(((train.loc[:,'feature_0']+1)//2).to_numpy(),dtype=torch.long,device=dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015179,
     "end_time": "2021-02-08T04:56:57.956277",
     "exception": false,
     "start_time": "2021-02-08T04:56:57.941098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We make a separate tensor that contains all other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:56:58.785845Z",
     "iopub.status.busy": "2021-02-08T04:56:58.785203Z",
     "iopub.status.idle": "2021-02-08T04:56:58.789865Z",
     "shell.execute_reply": "2021-02-08T04:56:58.789441Z"
    },
    "papermill": {
     "duration": 0.818295,
     "end_time": "2021-02-08T04:56:58.789962",
     "exception": false,
     "start_time": "2021-02-08T04:56:57.971667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = ['feature_'+str(i) for i in range(1,130)]\n",
    "train = train[feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016092,
     "end_time": "2021-02-08T04:56:58.821708",
     "exception": false,
     "start_time": "2021-02-08T04:56:58.805616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:56:58.864798Z",
     "iopub.status.busy": "2021-02-08T04:56:58.863934Z",
     "iopub.status.idle": "2021-02-08T04:57:55.381575Z",
     "shell.execute_reply": "2021-02-08T04:57:55.382687Z"
    },
    "papermill": {
     "duration": 56.545749,
     "end_time": "2021-02-08T04:57:55.382869",
     "exception": false,
     "start_time": "2021-02-08T04:56:58.837120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxindex = np.zeros((129,3))\n",
    "for i in range(129):\n",
    "    counts = train[feature_names[i]].value_counts()\n",
    "    mean = train[feature_names[i]].mean()\n",
    "    std = train[feature_names[i]].std()\n",
    "    sigmas = np.abs(counts.index[0]-mean)/std\n",
    "    maxindex[i] = [counts.index[0], counts.iloc[0], sigmas]\n",
    "    \n",
    "for i in range(129):\n",
    "    if maxindex[i,1] > 100 and maxindex[i,2] > 1:\n",
    "        train.replace({feature_names[i]: maxindex[i,0]},np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025065,
     "end_time": "2021-02-08T04:57:55.479257",
     "exception": false,
     "start_time": "2021-02-08T04:57:55.454192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we need to deal with NaN. We impute those missing values with the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:57:55.542938Z",
     "iopub.status.busy": "2021-02-08T04:57:55.542114Z",
     "iopub.status.idle": "2021-02-08T04:57:57.521434Z",
     "shell.execute_reply": "2021-02-08T04:57:57.520958Z"
    },
    "papermill": {
     "duration": 2.017083,
     "end_time": "2021-02-08T04:57:57.521544",
     "exception": false,
     "start_time": "2021-02-08T04:57:55.504461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fill_val=train.mean()\n",
    "train = train.fillna(fill_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015853,
     "end_time": "2021-02-08T04:57:57.553347",
     "exception": false,
     "start_time": "2021-02-08T04:57:57.537494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We remove features that correlate too strongly with feature_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:57:57.593273Z",
     "iopub.status.busy": "2021-02-08T04:57:57.592718Z",
     "iopub.status.idle": "2021-02-08T04:58:01.612289Z",
     "shell.execute_reply": "2021-02-08T04:58:01.611839Z"
    },
    "papermill": {
     "duration": 4.042922,
     "end_time": "2021-02-08T04:58:01.612421",
     "exception": false,
     "start_time": "2021-02-08T04:57:57.569499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr = train.corrwith(feature_0)\n",
    "remove_names = corr.loc[np.abs(corr) > 0.7].index\n",
    "train = train.drop(remove_names,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016499,
     "end_time": "2021-02-08T04:58:01.646972",
     "exception": false,
     "start_time": "2021-02-08T04:58:01.630473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We compute the principal components and reduce the feature space using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:58:01.685496Z",
     "iopub.status.busy": "2021-02-08T04:58:01.684536Z",
     "iopub.status.idle": "2021-02-08T04:59:05.917135Z",
     "shell.execute_reply": "2021-02-08T04:59:05.916082Z"
    },
    "papermill": {
     "duration": 64.254351,
     "end_time": "2021-02-08T04:59:05.917265",
     "exception": false,
     "start_time": "2021-02-08T04:58:01.662914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_components = 60\n",
    "sc = StandardScaler().fit(train.to_numpy())\n",
    "train = sc.transform(train.to_numpy())\n",
    "pca = PCA(n_components = pca_components).fit(train)\n",
    "train=pca.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015882,
     "end_time": "2021-02-08T04:59:05.949819",
     "exception": false,
     "start_time": "2021-02-08T04:59:05.933937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally we have a tensor with the last features we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:59:05.986388Z",
     "iopub.status.busy": "2021-02-08T04:59:05.985843Z",
     "iopub.status.idle": "2021-02-08T04:59:06.541636Z",
     "shell.execute_reply": "2021-02-08T04:59:06.541131Z"
    },
    "papermill": {
     "duration": 0.57595,
     "end_time": "2021-02-08T04:59:06.541747",
     "exception": false,
     "start_time": "2021-02-08T04:59:05.965797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = torch.tensor(train,dtype=torch.float,device=dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016624,
     "end_time": "2021-02-08T04:59:06.576969",
     "exception": false,
     "start_time": "2021-02-08T04:59:06.560345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016257,
     "end_time": "2021-02-08T04:59:06.610462",
     "exception": false,
     "start_time": "2021-02-08T04:59:06.594205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will make a very simple model at first using pytorch. The idea is to have fully connected layers deal with all of the floating point features, while feature_0 is used in an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:59:06.658853Z",
     "iopub.status.busy": "2021-02-08T04:59:06.657061Z",
     "iopub.status.idle": "2021-02-08T04:59:06.659470Z",
     "shell.execute_reply": "2021-02-08T04:59:06.659894Z"
    },
    "papermill": {
     "duration": 0.033288,
     "end_time": "2021-02-08T04:59:06.660006",
     "exception": false,
     "start_time": "2021-02-08T04:59:06.626718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "e_size = 64\n",
    "fc_input = pca_components\n",
    "h_dims = [512,512,256,128]\n",
    "dropout_rate = 0.5\n",
    "epochs = 200\n",
    "minibatch_size = 100000\n",
    "\n",
    "class MarketPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarketPredictor, self).__init__()\n",
    "        \n",
    "        self.e = nn.Embedding(2,e_size)\n",
    "        self.deep = nn.Sequential(\n",
    "            nn.Linear(fc_input,h_dims[0]),\n",
    "            nn.BatchNorm1d(h_dims[0]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(h_dims[0],h_dims[1]),\n",
    "            nn.BatchNorm1d(h_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(h_dims[1],h_dims[2]),\n",
    "            nn.BatchNorm1d(h_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(h_dims[2],h_dims[3]),\n",
    "            nn.BatchNorm1d(h_dims[3]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(h_dims[3],e_size),\n",
    "            nn.BatchNorm1d(e_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "            )\n",
    "        self.reduce = nn.utils.weight_norm(nn.Linear(e_size,5))\n",
    "        \n",
    "    def forward(self,xi,xf):\n",
    "        e_out = self.e(xi)\n",
    "        f_out = self.deep(xf)\n",
    "        ef_out = self.reduce(e_out+f_out)\n",
    "        \n",
    "        return ef_out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016053,
     "end_time": "2021-02-08T04:59:06.692266",
     "exception": false,
     "start_time": "2021-02-08T04:59:06.676213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we train it. Let's define the loss function first. In the competition we're told that the return on day $i$ is\n",
    "\\begin{equation}\n",
    "p_i = \\sum_j (\\mathit{weight}_{ij}*\\mathit{resp}_{ij}*\\mathit{action}_{ij})\n",
    "\\end{equation}\n",
    "This is essentially a problem about predicting the correct actions. The output of our model defines a probability distribution for the action, which is either $0$ or $1$. Now, we have a few strategies one is just to minimize the cost function, which would involve maximizing $p_i$, however, it is more efficient to minimize the [cross-entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) of the probability distribution defined by our network with respect to the target values of our data. During training we can then weigh this distribution if wish with the magnitude of the provided resp and weight of the training data if we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:59:06.729112Z",
     "iopub.status.busy": "2021-02-08T04:59:06.728588Z",
     "iopub.status.idle": "2021-02-08T04:59:06.732481Z",
     "shell.execute_reply": "2021-02-08T04:59:06.731984Z"
    },
    "papermill": {
     "duration": 0.024074,
     "end_time": "2021-02-08T04:59:06.732621",
     "exception": false,
     "start_time": "2021-02-08T04:59:06.708547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss().to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016284,
     "end_time": "2021-02-08T04:59:06.765691",
     "exception": false,
     "start_time": "2021-02-08T04:59:06.749407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's make some torch tensors which hold the training data and apply our model to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T04:59:06.813858Z",
     "iopub.status.busy": "2021-02-08T04:59:06.813281Z",
     "iopub.status.idle": "2021-02-08T04:59:06.872234Z",
     "shell.execute_reply": "2021-02-08T04:59:06.871696Z"
    },
    "papermill": {
     "duration": 0.089609,
     "end_time": "2021-02-08T04:59:06.872347",
     "exception": false,
     "start_time": "2021-02-08T04:59:06.782738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MarketPredictor().to(dev)\n",
    "opt = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-08T04:59:06.914618Z",
     "iopub.status.busy": "2021-02-08T04:59:06.914047Z",
     "iopub.status.idle": "2021-02-08T05:15:12.621242Z",
     "shell.execute_reply": "2021-02-08T05:15:12.620624Z"
    },
    "papermill": {
     "duration": 965.732524,
     "end_time": "2021-02-08T05:15:12.621404",
     "exception": false,
     "start_time": "2021-02-08T04:59:06.888880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0 / 200\n",
      "Loss is 0.703585147857666\n",
      "Epoch is 1 / 200\n",
      "Loss is 0.6952752470970154\n",
      "Epoch is 2 / 200\n",
      "Loss is 0.6931472420692444\n",
      "Epoch is 3 / 200\n",
      "Loss is 0.6919404864311218\n",
      "Epoch is 4 / 200\n",
      "Loss is 0.6913748979568481\n",
      "Epoch is 5 / 200\n",
      "Loss is 0.6912364959716797\n",
      "Epoch is 6 / 200\n",
      "Loss is 0.690486490726471\n",
      "Epoch is 7 / 200\n",
      "Loss is 0.6906060576438904\n",
      "Epoch is 8 / 200\n",
      "Loss is 0.6905030012130737\n",
      "Epoch is 9 / 200\n",
      "Loss is 0.6901842355728149\n",
      "Epoch is 10 / 200\n",
      "Loss is 0.689601480960846\n",
      "Epoch is 11 / 200\n",
      "Loss is 0.689899742603302\n",
      "Epoch is 12 / 200\n",
      "Loss is 0.6898735165596008\n",
      "Epoch is 13 / 200\n",
      "Loss is 0.6896629929542542\n",
      "Epoch is 14 / 200\n",
      "Loss is 0.6893044710159302\n",
      "Epoch is 15 / 200\n",
      "Loss is 0.6895025968551636\n",
      "Epoch is 16 / 200\n",
      "Loss is 0.6891094446182251\n",
      "Epoch is 17 / 200\n",
      "Loss is 0.6896559596061707\n",
      "Epoch is 18 / 200\n",
      "Loss is 0.6893314123153687\n",
      "Epoch is 19 / 200\n",
      "Loss is 0.6890257596969604\n",
      "Epoch is 20 / 200\n",
      "Loss is 0.6888669729232788\n",
      "Epoch is 21 / 200\n",
      "Loss is 0.6891071200370789\n",
      "Epoch is 22 / 200\n",
      "Loss is 0.6886296272277832\n",
      "Epoch is 23 / 200\n",
      "Loss is 0.6888402700424194\n",
      "Epoch is 24 / 200\n",
      "Loss is 0.6888763904571533\n",
      "Epoch is 25 / 200\n",
      "Loss is 0.6885893940925598\n",
      "Epoch is 26 / 200\n",
      "Loss is 0.6886205077171326\n",
      "Epoch is 27 / 200\n",
      "Loss is 0.6885993480682373\n",
      "Epoch is 28 / 200\n",
      "Loss is 0.6885983943939209\n",
      "Epoch is 29 / 200\n",
      "Loss is 0.688156008720398\n",
      "Epoch is 30 / 200\n",
      "Loss is 0.6880067586898804\n",
      "Epoch is 31 / 200\n",
      "Loss is 0.688332200050354\n",
      "Epoch is 32 / 200\n",
      "Loss is 0.6880687475204468\n",
      "Epoch is 33 / 200\n",
      "Loss is 0.6878818273544312\n",
      "Epoch is 34 / 200\n",
      "Loss is 0.6875720024108887\n",
      "Epoch is 35 / 200\n",
      "Loss is 0.6880453824996948\n",
      "Epoch is 36 / 200\n",
      "Loss is 0.687859296798706\n",
      "Epoch is 37 / 200\n",
      "Loss is 0.6877152919769287\n",
      "Epoch is 38 / 200\n",
      "Loss is 0.687935471534729\n",
      "Epoch is 39 / 200\n",
      "Loss is 0.6876376271247864\n",
      "Epoch is 40 / 200\n",
      "Loss is 0.6869780421257019\n",
      "Epoch is 41 / 200\n",
      "Loss is 0.6869173645973206\n",
      "Epoch is 42 / 200\n",
      "Loss is 0.6871768832206726\n",
      "Epoch is 43 / 200\n",
      "Loss is 0.6870946884155273\n",
      "Epoch is 44 / 200\n",
      "Loss is 0.6872859001159668\n",
      "Epoch is 45 / 200\n",
      "Loss is 0.6868942379951477\n",
      "Epoch is 46 / 200\n",
      "Loss is 0.686972975730896\n",
      "Epoch is 47 / 200\n",
      "Loss is 0.6872708201408386\n",
      "Epoch is 48 / 200\n",
      "Loss is 0.6868361830711365\n",
      "Epoch is 49 / 200\n",
      "Loss is 0.6866453886032104\n",
      "Epoch is 50 / 200\n",
      "Loss is 0.6866997480392456\n",
      "Epoch is 51 / 200\n",
      "Loss is 0.6864568591117859\n",
      "Epoch is 52 / 200\n",
      "Loss is 0.6863325834274292\n",
      "Epoch is 53 / 200\n",
      "Loss is 0.6864069700241089\n",
      "Epoch is 54 / 200\n",
      "Loss is 0.6861757040023804\n",
      "Epoch is 55 / 200\n",
      "Loss is 0.6863338947296143\n",
      "Epoch is 56 / 200\n",
      "Loss is 0.6865479946136475\n",
      "Epoch is 57 / 200\n",
      "Loss is 0.6862557530403137\n",
      "Epoch is 58 / 200\n",
      "Loss is 0.6856978535652161\n",
      "Epoch is 59 / 200\n",
      "Loss is 0.6864292621612549\n",
      "Epoch is 60 / 200\n",
      "Loss is 0.6859720349311829\n",
      "Epoch is 61 / 200\n",
      "Loss is 0.6854372620582581\n",
      "Epoch is 62 / 200\n",
      "Loss is 0.6855289936065674\n",
      "Epoch is 63 / 200\n",
      "Loss is 0.6853184103965759\n",
      "Epoch is 64 / 200\n",
      "Loss is 0.6851726174354553\n",
      "Epoch is 65 / 200\n",
      "Loss is 0.6856261491775513\n",
      "Epoch is 66 / 200\n",
      "Loss is 0.6854996085166931\n",
      "Epoch is 67 / 200\n",
      "Loss is 0.6854425072669983\n",
      "Epoch is 68 / 200\n",
      "Loss is 0.6851467490196228\n",
      "Epoch is 69 / 200\n",
      "Loss is 0.6855185031890869\n",
      "Epoch is 70 / 200\n",
      "Loss is 0.685104250907898\n",
      "Epoch is 71 / 200\n",
      "Loss is 0.6852133870124817\n",
      "Epoch is 72 / 200\n",
      "Loss is 0.6844673752784729\n",
      "Epoch is 73 / 200\n",
      "Loss is 0.6847128868103027\n",
      "Epoch is 74 / 200\n",
      "Loss is 0.6850083470344543\n",
      "Epoch is 75 / 200\n",
      "Loss is 0.6853349804878235\n",
      "Epoch is 76 / 200\n",
      "Loss is 0.6845876574516296\n",
      "Epoch is 77 / 200\n",
      "Loss is 0.6842464804649353\n",
      "Epoch is 78 / 200\n",
      "Loss is 0.6846707463264465\n",
      "Epoch is 79 / 200\n",
      "Loss is 0.6844602227210999\n",
      "Epoch is 80 / 200\n",
      "Loss is 0.6841238737106323\n",
      "Epoch is 81 / 200\n",
      "Loss is 0.6842874884605408\n",
      "Epoch is 82 / 200\n",
      "Loss is 0.684037983417511\n",
      "Epoch is 83 / 200\n",
      "Loss is 0.683829665184021\n",
      "Epoch is 84 / 200\n",
      "Loss is 0.6840461492538452\n",
      "Epoch is 85 / 200\n",
      "Loss is 0.6836187839508057\n",
      "Epoch is 86 / 200\n",
      "Loss is 0.6838775873184204\n",
      "Epoch is 87 / 200\n",
      "Loss is 0.6843848824501038\n",
      "Epoch is 88 / 200\n",
      "Loss is 0.6832817196846008\n",
      "Epoch is 89 / 200\n",
      "Loss is 0.6828228831291199\n",
      "Epoch is 90 / 200\n",
      "Loss is 0.6841080784797668\n",
      "Epoch is 91 / 200\n",
      "Loss is 0.6832859516143799\n",
      "Epoch is 92 / 200\n",
      "Loss is 0.6832777857780457\n",
      "Epoch is 93 / 200\n",
      "Loss is 0.6830337643623352\n",
      "Epoch is 94 / 200\n",
      "Loss is 0.6829472780227661\n",
      "Epoch is 95 / 200\n",
      "Loss is 0.6828888654708862\n",
      "Epoch is 96 / 200\n",
      "Loss is 0.6836504340171814\n",
      "Epoch is 97 / 200\n",
      "Loss is 0.6836068034172058\n",
      "Epoch is 98 / 200\n",
      "Loss is 0.6830263733863831\n",
      "Epoch is 99 / 200\n",
      "Loss is 0.6827913522720337\n",
      "Epoch is 100 / 200\n",
      "Loss is 0.6825438737869263\n",
      "Epoch is 101 / 200\n",
      "Loss is 0.6829380393028259\n",
      "Epoch is 102 / 200\n",
      "Loss is 0.6828290224075317\n",
      "Epoch is 103 / 200\n",
      "Loss is 0.6830772757530212\n",
      "Epoch is 104 / 200\n",
      "Loss is 0.6823099851608276\n",
      "Epoch is 105 / 200\n",
      "Loss is 0.6823132038116455\n",
      "Epoch is 106 / 200\n",
      "Loss is 0.6823061108589172\n",
      "Epoch is 107 / 200\n",
      "Loss is 0.6820412278175354\n",
      "Epoch is 108 / 200\n",
      "Loss is 0.6823132038116455\n",
      "Epoch is 109 / 200\n",
      "Loss is 0.6816083788871765\n",
      "Epoch is 110 / 200\n",
      "Loss is 0.6827183961868286\n",
      "Epoch is 111 / 200\n",
      "Loss is 0.6814928650856018\n",
      "Epoch is 112 / 200\n",
      "Loss is 0.6822987198829651\n",
      "Epoch is 113 / 200\n",
      "Loss is 0.6818745136260986\n",
      "Epoch is 114 / 200\n",
      "Loss is 0.6818466782569885\n",
      "Epoch is 115 / 200\n",
      "Loss is 0.6813863515853882\n",
      "Epoch is 116 / 200\n",
      "Loss is 0.6815946102142334\n",
      "Epoch is 117 / 200\n",
      "Loss is 0.681483268737793\n",
      "Epoch is 118 / 200\n",
      "Loss is 0.6815003752708435\n",
      "Epoch is 119 / 200\n",
      "Loss is 0.6817943453788757\n",
      "Epoch is 120 / 200\n",
      "Loss is 0.6815676689147949\n",
      "Epoch is 121 / 200\n",
      "Loss is 0.6811012029647827\n",
      "Epoch is 122 / 200\n",
      "Loss is 0.6814078688621521\n",
      "Epoch is 123 / 200\n",
      "Loss is 0.6813840866088867\n",
      "Epoch is 124 / 200\n",
      "Loss is 0.6816664934158325\n",
      "Epoch is 125 / 200\n",
      "Loss is 0.6821824312210083\n",
      "Epoch is 126 / 200\n",
      "Loss is 0.6817170977592468\n",
      "Epoch is 127 / 200\n",
      "Loss is 0.6803973913192749\n",
      "Epoch is 128 / 200\n",
      "Loss is 0.6821632385253906\n",
      "Epoch is 129 / 200\n",
      "Loss is 0.6808972358703613\n",
      "Epoch is 130 / 200\n",
      "Loss is 0.6808720231056213\n",
      "Epoch is 131 / 200\n",
      "Loss is 0.6807134747505188\n",
      "Epoch is 132 / 200\n",
      "Loss is 0.6820701956748962\n",
      "Epoch is 133 / 200\n",
      "Loss is 0.6808021068572998\n",
      "Epoch is 134 / 200\n",
      "Loss is 0.6816974878311157\n",
      "Epoch is 135 / 200\n",
      "Loss is 0.6812713742256165\n",
      "Epoch is 136 / 200\n",
      "Loss is 0.6804980635643005\n",
      "Epoch is 137 / 200\n",
      "Loss is 0.6806360483169556\n",
      "Epoch is 138 / 200\n",
      "Loss is 0.6808393001556396\n",
      "Epoch is 139 / 200\n",
      "Loss is 0.6808153986930847\n",
      "Epoch is 140 / 200\n",
      "Loss is 0.6807414889335632\n",
      "Epoch is 141 / 200\n",
      "Loss is 0.6811153888702393\n",
      "Epoch is 142 / 200\n",
      "Loss is 0.6808882355690002\n",
      "Epoch is 143 / 200\n",
      "Loss is 0.680897057056427\n",
      "Epoch is 144 / 200\n",
      "Loss is 0.6801621317863464\n",
      "Epoch is 145 / 200\n",
      "Loss is 0.6803185939788818\n",
      "Epoch is 146 / 200\n",
      "Loss is 0.68021160364151\n",
      "Epoch is 147 / 200\n",
      "Loss is 0.6803684234619141\n",
      "Epoch is 148 / 200\n",
      "Loss is 0.6803581714630127\n",
      "Epoch is 149 / 200\n",
      "Loss is 0.680091381072998\n",
      "Epoch is 150 / 200\n",
      "Loss is 0.6803017258644104\n",
      "Epoch is 151 / 200\n",
      "Loss is 0.6797323226928711\n",
      "Epoch is 152 / 200\n",
      "Loss is 0.6802273392677307\n",
      "Epoch is 153 / 200\n",
      "Loss is 0.679989755153656\n",
      "Epoch is 154 / 200\n",
      "Loss is 0.6805428862571716\n",
      "Epoch is 155 / 200\n",
      "Loss is 0.6800851821899414\n",
      "Epoch is 156 / 200\n",
      "Loss is 0.6817386150360107\n",
      "Epoch is 157 / 200\n",
      "Loss is 0.6807023882865906\n",
      "Epoch is 158 / 200\n",
      "Loss is 0.6797338724136353\n",
      "Epoch is 159 / 200\n",
      "Loss is 0.680156409740448\n",
      "Epoch is 160 / 200\n",
      "Loss is 0.6794635057449341\n",
      "Epoch is 161 / 200\n",
      "Loss is 0.679204523563385\n",
      "Epoch is 162 / 200\n",
      "Loss is 0.6799863576889038\n",
      "Epoch is 163 / 200\n",
      "Loss is 0.6794430017471313\n",
      "Epoch is 164 / 200\n",
      "Loss is 0.6798988580703735\n",
      "Epoch is 165 / 200\n",
      "Loss is 0.6797946095466614\n",
      "Epoch is 166 / 200\n",
      "Loss is 0.6792423725128174\n",
      "Epoch is 167 / 200\n",
      "Loss is 0.6795876026153564\n",
      "Epoch is 168 / 200\n",
      "Loss is 0.6792045831680298\n",
      "Epoch is 169 / 200\n",
      "Loss is 0.6796050071716309\n",
      "Epoch is 170 / 200\n",
      "Loss is 0.6801247596740723\n",
      "Epoch is 171 / 200\n",
      "Loss is 0.679485023021698\n",
      "Epoch is 172 / 200\n",
      "Loss is 0.6795353889465332\n",
      "Epoch is 173 / 200\n",
      "Loss is 0.6788753271102905\n",
      "Epoch is 174 / 200\n",
      "Loss is 0.6796432733535767\n",
      "Epoch is 175 / 200\n",
      "Loss is 0.6793898940086365\n",
      "Epoch is 176 / 200\n",
      "Loss is 0.6794509291648865\n",
      "Epoch is 177 / 200\n",
      "Loss is 0.6788667440414429\n",
      "Epoch is 178 / 200\n",
      "Loss is 0.6794543266296387\n",
      "Epoch is 179 / 200\n",
      "Loss is 0.6786811351776123\n",
      "Epoch is 180 / 200\n",
      "Loss is 0.6793649792671204\n",
      "Epoch is 181 / 200\n",
      "Loss is 0.6792936325073242\n",
      "Epoch is 182 / 200\n",
      "Loss is 0.6797235608100891\n",
      "Epoch is 183 / 200\n",
      "Loss is 0.6788317561149597\n",
      "Epoch is 184 / 200\n",
      "Loss is 0.6788442134857178\n",
      "Epoch is 185 / 200\n",
      "Loss is 0.6788583993911743\n",
      "Epoch is 186 / 200\n",
      "Loss is 0.6792041063308716\n",
      "Epoch is 187 / 200\n",
      "Loss is 0.6791920065879822\n",
      "Epoch is 188 / 200\n",
      "Loss is 0.6790721416473389\n",
      "Epoch is 189 / 200\n",
      "Loss is 0.6791108250617981\n",
      "Epoch is 190 / 200\n",
      "Loss is 0.678814172744751\n",
      "Epoch is 191 / 200\n",
      "Loss is 0.6786690354347229\n",
      "Epoch is 192 / 200\n",
      "Loss is 0.6789949536323547\n",
      "Epoch is 193 / 200\n",
      "Loss is 0.6790546178817749\n",
      "Epoch is 194 / 200\n",
      "Loss is 0.6794676184654236\n",
      "Epoch is 195 / 200\n",
      "Loss is 0.6789451837539673\n",
      "Epoch is 196 / 200\n",
      "Loss is 0.6790212392807007\n",
      "Epoch is 197 / 200\n",
      "Loss is 0.6784627437591553\n",
      "Epoch is 198 / 200\n",
      "Loss is 0.6780259013175964\n",
      "Epoch is 199 / 200\n",
      "Loss is 0.6798406839370728\n"
     ]
    }
   ],
   "source": [
    "minibatches = batch_size//minibatch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    permutation = torch.randperm(batch_size)\n",
    "    print('Epoch is',i,'/',epochs)\n",
    "    for j in range(minibatches):\n",
    "        opt.zero_grad()\n",
    "        s = model(itensor[permutation[j*minibatch_size:(j+1)*minibatch_size]],train[permutation[j*minibatch_size:(j+1)*minibatch_size]])\n",
    "        c = loss(s,target[permutation[j*minibatch_size:(j+1)*minibatch_size]])\n",
    "        c.backward()\n",
    "        opt.step()\n",
    "    print('Loss is',c.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.07251,
     "end_time": "2021-02-08T05:15:12.766776",
     "exception": false,
     "start_time": "2021-02-08T05:15:12.694266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Saving the model\n",
    "It's pretty easy to save a pytorch model. We will use pickle and save the state dict of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T05:15:12.920540Z",
     "iopub.status.busy": "2021-02-08T05:15:12.919965Z",
     "iopub.status.idle": "2021-02-08T05:15:12.946456Z",
     "shell.execute_reply": "2021-02-08T05:15:12.945984Z"
    },
    "papermill": {
     "duration": 0.107094,
     "end_time": "2021-02-08T05:15:12.946574",
     "exception": false,
     "start_time": "2021-02-08T05:15:12.839480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'marketpredictor_state_dict_'+str(epochs)+'epochs.pt'\n",
    "torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.072723,
     "end_time": "2021-02-08T05:15:13.091225",
     "exception": false,
     "start_time": "2021-02-08T05:15:13.018502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will also need the standard scaler and pca objects, as well as the maxindex and fill_val for when we run things for submission later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T05:15:13.246202Z",
     "iopub.status.busy": "2021-02-08T05:15:13.245302Z",
     "iopub.status.idle": "2021-02-08T05:15:13.249364Z",
     "shell.execute_reply": "2021-02-08T05:15:13.248826Z"
    },
    "papermill": {
     "duration": 0.086078,
     "end_time": "2021-02-08T05:15:13.249458",
     "exception": false,
     "start_time": "2021-02-08T05:15:13.163380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('feature_processing.pkl','wb') as f:\n",
    "    pickle.dump([sc,pca,maxindex,fill_val,remove_names],f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 1246.110315,
   "end_time": "2021-02-08T05:15:13.832896",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-08T04:54:27.722581",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
