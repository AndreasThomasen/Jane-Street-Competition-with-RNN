{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and feature selection of the Jane Street Market Prediction Competition Data\n",
    "\n",
    "In machine learning applications preprocessing of data and feature reduction is extremely important. Firstly it allows models to run on data with much lower dimension. This enables them to train faster and may even reduce randomness in the data, which makes predictions hard. Additionally, much of the raw data provided in real world scenarios have imperfections such as missing entries or NaN, etc.\n",
    "\n",
    "We here go into some detail with the training set provided by Jane Street in their Market Prediction Competition. This notebook can be summarized as follows.\n",
    "* We will first look at the data to determine which features are heavily correlated so that we can reduce the dimensionality of the data.\n",
    "* We discuss some of the results and possible strategies\n",
    "* We then reduce the feature space by PCA followed by T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the training data. This will take a while, so make yourself comfortable meanwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if there are any NaN or similar entries in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems at least some of the features have some invalid or missing entries. We have to find a way to deal with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first pick out a subset of the data for analysis since the training set is rather large. This notebook is for demonstration purposes, so we will only pick out 10000 samples to make things run relatively fast. I highly recommend using the full training set for a rigorous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train[0:9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"notebook\", font_scale=2.5):\n",
    "    g = sns.pairplot(train_subset[['feature_0','feature_1','feature_2','feature_3','feature_4','feature_5','feature_6','feature_7','date']],\n",
    "                     hue='date', palette='tab20', height=6)\n",
    "\n",
    "g.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9,9))\n",
    "plt.title(\"Correlation heat map\")\n",
    "sns.heatmap(train_subset.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very clear that the data is highly correlated. There are several blocks that could probably be collapsed into one-another. In order to deal with this we have will use T-SNE feature reduction method. Feature_0 looks special since it is integer and either 1 or -1. Let's examine its correlation with the feature 17 - 26 block more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"notebook\", font_scale=2.5):\n",
    "    g = sns.pairplot(train_subset[['feature_0','feature_17','feature_18','feature_19',\n",
    "                                   'feature_20','feature_21','feature_22','feature_23','feature_24','feature_25','feature_26','date']],\n",
    "                     hue='date', palette='tab20', height=6)\n",
    "\n",
    "g.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It again seems highly correlated with these features. It could be a feasible strategy to remove this feature entirely, or perhaps incorporate it using the embedding class of torch.nn while removing all of the other features above. Alternatively one could systematically remove features according to how well they correlate with feature_0 or something similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the data\n",
    "We will start by scaling the data using standard methods. Firstly we impute the data, i.e. remove missing values such as NaN and replace them with some numerical value that is compatible with our later processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val=train_subset.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace missing values by the mean of that column. The reason for this is that we will later process the data with T-SNE which is sensitive to outliers, so the best way to reduce the effect of this new value is to just have it be the exact mean of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_imputed = train_subset.fillna(fill_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of data, so initial feature reduction using PCA seems like a good first step. We will reduce the number of features to 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['feature_'+str(i) for i in range(130)]\n",
    "features = train_subset_imputed[feature_names]\n",
    "sc = StandardScaler().fit(features.to_numpy())\n",
    "features_scaled = sc.transform(features.to_numpy())\n",
    "pca = PCA(n_components = 40)\n",
    "features_pca=pca.fit_transform(features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do the T-SNE feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_embedded = TSNE(n_components = 3).fit_transform(features_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We now reduced the dataset to 3 components using PCA and T-SNE. It may be worthwhile to include more than just these three features to properly represent the data. I have a few comments to make about this.\n",
    "* T-SNE is a computationally expensive feature reduction scheme. Here we have used the Barnes-Hull method, which is approximate and only works for an output dimension of 2 or 3. However, the reason we used this is that it takes O(NlogN) to run. We could have chosen d > 3, but that would involve using the exact method, which scales as O(N^2). This becomes infeasible in practice with datasets like the present one. Perhaps if sufficient reduction of the data-set was done beforehand this would be a feasible way to transform the data.\n",
    "* One could manually extract feature_0 and exempt it from the feature extraction above thereby retaining a fourth feature and eliminating some of the features that correlate highly with feature_0 before PCA. Perhaps I will do this in an update to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is just for testing... will make an actual submission later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janestreet\n",
    "env = janestreet.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    sample_prediction_df.action = 0\n",
    "    env.predict(sample_prediction_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
